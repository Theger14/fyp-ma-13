{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d1930f2",
   "metadata": {},
   "source": [
    "## Steps\n",
    "### Step 1: Import and install required libraries\n",
    "### Step 2: Load data\n",
    "- Training data without pertubation\n",
    "- Training data with pertubation\n",
    "- Validation data\n",
    "\n",
    "### Step 3: Visualize Datasets\n",
    "- Are the datasets balanced?\n",
    "\n",
    "### Step 4: Data Preprocessing\n",
    "- Normalize/Transform the data\n",
    "- Dividing an image by 255 rescales the image from 255 to 0-1\n",
    "- Reshaping x_train and x_val\n",
    "\n",
    "### Step 5: Data Augmentation\n",
    "https://keras.io/api/preprocessing/image/\n",
    "- Generate batches of tensor image data with real-time data augmentation.\n",
    "\n",
    "### Step 6: Setting the Model\n",
    "- Convolution Layer and Max Pool 1 \n",
    "- Convolution Layer and Max Pool 2\n",
    "- Dropout Layer\n",
    "    - https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout\n",
    "- Flatten to a 1D Tensor\n",
    "- Dense Output Layer with 2 nodes with softmax activation\n",
    "    - Returns probability of an image belonging to a class \n",
    "\n",
    "### Step 7: Model Evaluation and Predictions\n",
    "- Accuracy and Loss\n",
    "- Confusion Matrix\n",
    "- Classification Report by sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e678747",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c974fa7a",
   "metadata": {},
   "source": [
    "preprocessing/\n",
    "... train_data/\n",
    "......female\n",
    "......male\n",
    "...val_data/\n",
    "......female\n",
    "...... male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1343607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install opencv-python\n",
    "# %pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32cfdc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout \n",
    "import seaborn as sns\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7734110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 8\n",
    "batch_size = 16\n",
    "img_size = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47c3e3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['female', 'male']\n",
    "\n",
    "def get_data(data_dir):\n",
    "    data = [] \n",
    "    for label in labels: \n",
    "        path = os.path.join(data_dir, label)\n",
    "        class_num = labels.index(label)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_arr = cv2.imread(os.path.join(path, img))[...,::-1] #convert BGR to RGB format\n",
    "                resized_arr = cv2.resize(img_arr, (img_size, img_size)) # Reshaping images to preferred size\n",
    "                data.append([resized_arr, class_num])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cce2375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n",
      "'NoneType' object is not subscriptable\n"
     ]
    }
   ],
   "source": [
    "train = get_data('preprocessing/train_data')\n",
    "val = get_data('preprocessing/val_data')\n",
    "train_pert = get_data('preprocessing/makeup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9989de21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "l = []\n",
    "for i in train:\n",
    "    if(i[1] == 0):\n",
    "        l.append(\"female_train\")\n",
    "    else:\n",
    "        l.append(\"male_train\")\n",
    "for j in val:\n",
    "    if(j[1] == 0):\n",
    "        l.append(\"female_val\")\n",
    "    else:\n",
    "        l.append(\"male_val\")\n",
    "sns.set_style('darkgrid')\n",
    "g = sns.countplot(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356606d0",
   "metadata": {},
   "source": [
    "# Without Pertubations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a118215e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing / Feature Engineering\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_val = []\n",
    "y_val = []\n",
    "\n",
    "for feature, label in train:\n",
    "    x_train.append(feature)\n",
    "    y_train.append(label)\n",
    "\n",
    "for feature, label in val:\n",
    "    x_val.append(feature)\n",
    "    y_val.append(label)\n",
    "\n",
    "# Normalize the data\n",
    "x_train = np.array(x_train) / 255\n",
    "x_val = np.array(x_val) / 255\n",
    "\n",
    "x_train.reshape(-1, img_size, img_size, 1)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "x_val.reshape(-1, img_size, img_size, 1)\n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ae6125c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.2, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip = True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9d83aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 75, 75, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 37, 37, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 13, 13, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 4610      \n",
      "=================================================================\n",
      "Total params: 24,002\n",
      "Trainable params: 24,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/8\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.7003 - accuracy: 0.4767\n",
      "Epoch 00001: loss improved from inf to 0.70033, saving model to model_best_weights.h5\n",
      "19/19 [==============================] - 3s 160ms/step - loss: 0.7003 - accuracy: 0.4767 - val_loss: 0.6982 - val_accuracy: 0.4828\n",
      "Epoch 2/8\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6975 - accuracy: 0.5033\n",
      "Epoch 00002: loss improved from 0.70033 to 0.69747, saving model to model_best_weights.h5\n",
      "19/19 [==============================] - 2s 122ms/step - loss: 0.6975 - accuracy: 0.5033 - val_loss: 0.6980 - val_accuracy: 0.4828\n",
      "Epoch 3/8\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6936 - accuracy: 0.5133\n",
      "Epoch 00003: loss improved from 0.69747 to 0.69357, saving model to model_best_weights.h5\n",
      "19/19 [==============================] - 2s 112ms/step - loss: 0.6936 - accuracy: 0.5133 - val_loss: 0.6979 - val_accuracy: 0.4828\n",
      "Epoch 4/8\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6964 - accuracy: 0.5233\n",
      "Epoch 00004: loss did not improve from 0.69357\n",
      "19/19 [==============================] - 2s 123ms/step - loss: 0.6964 - accuracy: 0.5233 - val_loss: 0.6978 - val_accuracy: 0.4828\n",
      "Epoch 5/8\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.7004 - accuracy: 0.4684\n",
      "Epoch 00005: loss did not improve from 0.69357\n",
      "19/19 [==============================] - 2s 116ms/step - loss: 0.7004 - accuracy: 0.4684 - val_loss: 0.6977 - val_accuracy: 0.4828\n",
      "Epoch 6/8\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.7004 - accuracy: 0.4934\n",
      "Epoch 00006: loss did not improve from 0.69357\n",
      "19/19 [==============================] - 2s 131ms/step - loss: 0.7004 - accuracy: 0.4934 - val_loss: 0.6975 - val_accuracy: 0.4828\n",
      "Epoch 7/8\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6987 - accuracy: 0.5150\n",
      "Epoch 00007: loss did not improve from 0.69357\n",
      "19/19 [==============================] - 2s 119ms/step - loss: 0.6987 - accuracy: 0.5150 - val_loss: 0.6975 - val_accuracy: 0.4828\n",
      "Epoch 8/8\n",
      "19/19 [==============================] - ETA: 0s - loss: 0.6971 - accuracy: 0.5050\n",
      "Epoch 00008: loss did not improve from 0.69357\n",
      "19/19 [==============================] - 2s 114ms/step - loss: 0.6971 - accuracy: 0.5050 - val_loss: 0.6973 - val_accuracy: 0.4845\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Convolutional layer 1\n",
    "model.add(Conv2D(32, 3, 3, padding=\"same\", activation=\"relu\", input_shape=(224,224,3)))\n",
    "model.add(MaxPool2D())\n",
    "\n",
    "# Convolutional layer 2\n",
    "model.add(Conv2D(64, 3, 3, padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPool2D())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "model.summary()\n",
    "#-----------------------------------------------------------------\n",
    "opt = Adam(lr=0.000001)\n",
    "model.compile(optimizer = opt , \n",
    "              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics = ['accuracy'])\n",
    "#------------------------------------------------------------------\n",
    "# early_stop = EarlyStopping(monitor='loss', min_delta=0.001, patience=3, mode='min', verbose=1)\n",
    "checkpoint = ModelCheckpoint('model_best_weights.h5', monitor='loss', verbose=1, # Saves checkpoints\n",
    "                             save_best_only=True, mode='min', save_freq='epoch')\n",
    "\n",
    "\n",
    "history = model.fit(x_train,y_train,epochs = epochs , \n",
    "                    validation_data = (x_val, y_val), \n",
    "                    callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b01be6b",
   "metadata": {},
   "source": [
    "# With Pertubations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37e96ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "x_val = []\n",
    "y_val = []\n",
    "\n",
    "for feature, label in train_pert:\n",
    "    x_train.append(feature)\n",
    "    y_train.append(label)\n",
    "\n",
    "for feature, label in val:\n",
    "    x_val.append(feature)\n",
    "    y_val.append(label)\n",
    "\n",
    "# Normalize the data\n",
    "x_train = np.array(x_train) / 255\n",
    "x_val = np.array(x_val) / 255\n",
    "\n",
    "x_train.reshape(-1, img_size, img_size, 1)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "x_val.reshape(-1, img_size, img_size, 1)\n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cadd8e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range = 30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.2, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip = True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a104d4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 75, 75, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 37, 37, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 13, 13, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 4610      \n",
      "=================================================================\n",
      "Total params: 24,002\n",
      "Trainable params: 24,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/8\n",
      " 9/14 [==================>...........] - ETA: 0s - loss: 0.6909 - accuracy: 0.5486"
     ]
    }
   ],
   "source": [
    "model_pert = Sequential()\n",
    "\n",
    "model_pert.add(Conv2D(32, 3, 3, padding=\"same\", activation=\"relu\", input_shape=(224,224,3)))\n",
    "model_pert.add(MaxPool2D())\n",
    "\n",
    "model_pert.add(Conv2D(64, 3, 3, padding=\"same\", activation=\"relu\"))\n",
    "model_pert.add(MaxPool2D())\n",
    "model_pert.add(Dropout(0.4))\n",
    "\n",
    "model_pert.add(Flatten())\n",
    "model_pert.add(Dense(2, activation=\"softmax\"))\n",
    "\n",
    "model_pert.summary()\n",
    "#-----------------------------------------------------------------\n",
    "opt = Adam(lr=0.000001)\n",
    "model_pert.compile(optimizer = opt , \n",
    "              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics = ['accuracy'])\n",
    "#------------------------------------------------------------------\n",
    "# early_stop = EarlyStopping(monitor='loss', min_delta=0.001, patience=3, mode='min', verbose=1)\n",
    "checkpoint = ModelCheckpoint('model_best_weights_pert.h5', monitor='loss', verbose=1, # Saves checkpoints\n",
    "                             save_best_only=True, mode='min', save_freq='epoch')\n",
    "\n",
    "history_pert = model_pert.fit(x_train,y_train,epochs = epochs , \n",
    "                    validation_data = (x_val, y_val), \n",
    "                    callbacks = [checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b907840a",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0f5bc3",
   "metadata": {},
   "source": [
    "## Without Pertubation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410fc048",
   "metadata": {},
   "source": [
    "### Accuracy and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51608b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3401fc",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fbbab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_train_samples = 602\n",
    "num_of_test_samples = 582"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11432b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609db824",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_generator = test_datagen.flow_from_directory(\"preprocessing/val_data\",\n",
    "                                                        target_size=(224, 224),\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa4e5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict_generator(validation_generator, num_of_test_samples // batch_size+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "print('Confusion Matrix')\n",
    "confusion_matrix_array = confusion_matrix(validation_generator.classes, y_pred)\n",
    "print(confusion_matrix_array)\n",
    "df_cm = pd.DataFrame(confusion_matrix_array, range(2), range(2))\n",
    "# plt.figure(figsize=(10,7))\n",
    "sns.set(font_scale=1.4) # for label size\n",
    "sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 12}) # font size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca73e2d7",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd901c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(x_val)\n",
    "predictions = predictions.reshape(1,-1)[0]\n",
    "\n",
    "target_names = ['Female (Class 0)','Male (Class 1)']\n",
    "print(classification_report(validation_generator.classes, y_pred, target_names=target_names, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1b22e4",
   "metadata": {},
   "source": [
    "## With Pertubation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a131b8a",
   "metadata": {},
   "source": [
    "### Accuracy and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1608e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history_pert.history['accuracy']\n",
    "val_acc = history_pert.history['val_accuracy']\n",
    "loss = history_pert.history['loss']\n",
    "val_loss = history_pert.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdc6116",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4e004d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_train_samples = 602\n",
    "num_of_test_samples = 582"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57345e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd8db0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_generator = test_datagen.flow_from_directory(\"preprocessing/val_data\",\n",
    "                                                        target_size=(224, 224),\n",
    "                                                        batch_size=batch_size,\n",
    "                                                        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be850af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model_pert.predict_generator(validation_generator, num_of_test_samples // batch_size+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "print('Confusion Matrix')\n",
    "confusion_matrix_array = confusion_matrix(validation_generator.classes, y_pred)\n",
    "print(confusion_matrix_array)\n",
    "df_cm = pd.DataFrame(confusion_matrix_array, range(2), range(2))\n",
    "# plt.figure(figsize=(10,7))\n",
    "sns.set(font_scale=1.4) # for label size\n",
    "sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 12}) # font size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5517aeb",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea62af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_pert.predict_classes(x_val)\n",
    "predictions = predictions.reshape(1,-1)[0]\n",
    "\n",
    "target_names = ['Female (Class 0)','Male (Class 1)']\n",
    "print(classification_report(validation_generator.classes, y_pred, target_names=target_names, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2486b17",
   "metadata": {},
   "source": [
    "# Making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b346356",
   "metadata": {},
   "source": [
    "## Without Pertubation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5161c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictImage(filename):\n",
    "    img = tf.keras.preprocessing.image.load_img(filename, target_size=(224, 224))\n",
    "    input_arr = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    input_arr = np.array([input_arr])  # Convert single image to a batch.\n",
    "    input_arr = input_arr.astype('float32') / 255.  # This is VERY important\n",
    "    predictions = model.predict(input_arr)\n",
    "    val = np.argmax(predictions, axis=-1)\n",
    "    \n",
    "    img1 = image.load_img(filename,target_size=(img_size,img_size))\n",
    "    plt.imshow(img1)\n",
    "    if val == 1:\n",
    "        plt.xlabel(\"MALE\",fontsize=30)\n",
    "    elif val == 0:\n",
    "        plt.xlabel(\"FEMALE\",fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1d3917",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictImage(\"trump.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb65bbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictImage(\"gal.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f07b4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictImage(\"clairo.jpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1d5aad",
   "metadata": {},
   "source": [
    "## With Pertubation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486bb263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictImagePert(filename):\n",
    "    img = tf.keras.preprocessing.image.load_img(filename, target_size=(224, 224))\n",
    "    input_arr = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    input_arr = np.array([input_arr])  # Convert single image to a batch.\n",
    "    input_arr = input_arr.astype('float32') / 255.  # This is VERY important\n",
    "    predictions = model_pert.predict(input_arr)\n",
    "    val = np.argmax(predictions, axis=-1)\n",
    "    \n",
    "    img1 = image.load_img(filename,target_size=(img_size,img_size))\n",
    "    plt.imshow(img1)\n",
    "    if val == 1:\n",
    "        plt.xlabel(\"MALE\",fontsize=30)\n",
    "    elif val == 0:\n",
    "        plt.xlabel(\"FEMALE\",fontsize=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa53d40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictImagePert(\"trump.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7022f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictImagePert(\"gal.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3345ea84",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictImagePert(\"clairo.jpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cbe825",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c899357b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow2_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
